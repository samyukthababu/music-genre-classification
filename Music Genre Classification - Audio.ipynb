{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MI0DvlwqCSnE"
      },
      "outputs": [],
      "source": [
        "# Import Statements\n",
        "import os\n",
        "import librosa\n",
        "import librosa.display\n",
        "import numpy as np\n",
        "from numpy import argmax\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import Concatenate\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras.layers import Embedding\n",
        "import IPython.display as ipd\n",
        "from scipy.io.wavfile import write\n",
        "import soundfile as sf\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxAdrNSLCxaK"
      },
      "outputs": [],
      "source": [
        "# Setting the seed to ensure reproducibility\n",
        "from numpy.random import seed\n",
        "seed(1)\n",
        "import tensorflow\n",
        "tensorflow.random.set_seed(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JB2h3In7dLk"
      },
      "outputs": [],
      "source": [
        "# Loading the drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dAmyBHlHG6H"
      },
      "source": [
        "**Loading the Dataset (Audio files):**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rN2nbbg3C8Fs"
      },
      "outputs": [],
      "source": [
        "# The location of the original dataset in the Drive\n",
        "\n",
        "path = \"/content/drive/MyDrive/genres_original\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRG9wm40DZzO"
      },
      "outputs": [],
      "source": [
        "def load_dataset(path):\n",
        "\n",
        "  # All the 10 classes is stored in the variable \"classes\"\n",
        "  classes = []\n",
        "  X = []\n",
        "  y = []\n",
        "  num_segments = 10\n",
        "  sample_rate_for_3sec = 22050 * 3\n",
        "\n",
        "  # Audio files (30 seconds) are loaded using librosa and split as 3 second files\n",
        "  for root, dir, files in os.walk(path):\n",
        "\n",
        "    if root != path:\n",
        "      classes.append(root.split(\"/\")[-1])\n",
        "\n",
        "      for fp in files:\n",
        "\n",
        "        filepath = root + \"/\" + fp\n",
        "\n",
        "        # There is an error with this audio file, so it has just been skipped and all other files have been read\n",
        "        # The total number of 30 second audio files are 999 instead of 1000\n",
        "        if filepath != \"/content/drive/MyDrive/genres_original/jazz/jazz.00054.wav\":\n",
        "\n",
        "          # Librosa.load returns a time series representation of the audio signal and the sampling rate\n",
        "          # Sample rate = 22,050 per second\n",
        "          # Since there are 30 seconds in a audio signal, the total sample rate will be 30 * 22,050 = 661,500\n",
        "          # waveform shape => (661500,) => for 30 second audio file\n",
        "          waveform, sample_rate = librosa.load(filepath, duration = 30)\n",
        "\n",
        "          # If the audio file is not 30 seconds long, we will not process it\n",
        "          if waveform.shape[0] / sample_rate == 30:\n",
        "\n",
        "            # Splitting into 3 second audio files\n",
        "            for i in range(num_segments):\n",
        "              start_segment = int(sample_rate_for_3sec * i)\n",
        "              end_segment = int(start_segment + sample_rate_for_3sec)\n",
        "              X.append(waveform[start_segment:end_segment])\n",
        "              y.append(root.split(\"/\")[-1])\n",
        "  return X, y, classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5dU2xAaVEhAH"
      },
      "outputs": [],
      "source": [
        "# X is the time series represebtation of the input audio file with a shape of => (9900, 66150)\n",
        "# y is the genre of the audio file => length of 9900\n",
        "# classes => length of 10\n",
        "X, y_cat, classes = load_dataset(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0BpzI_1GJNc"
      },
      "outputs": [],
      "source": [
        "# Encoding the categories using numbers\n",
        "y = []\n",
        "\n",
        "for i in range(len(y_cat)):\n",
        "  y.append(classes.index(y_cat[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nB3B09aVGs58"
      },
      "source": [
        "**Preparing the Training, Validation and Test Set:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_n8qzFooGa5n"
      },
      "outputs": [],
      "source": [
        "# X shape => (9900, 66150)\n",
        "X = np.array(X)\n",
        "# y shape => (9900,)\n",
        "y = np.array(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imibdCCBGlxf"
      },
      "outputs": [],
      "source": [
        "# trainX shape => (7128, 66150)\n",
        "# valX shape => (1782, 66150)\n",
        "# testX shape => (990, 66150)\n",
        "trainX, testX, trainy, testy = train_test_split(X, y, test_size = 0.1)\n",
        "trainX, valX, trainy, valy = train_test_split(trainX, trainy, test_size = 0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Vf3TxDwHO2h"
      },
      "source": [
        "**Mel-frequency cepstral coefficients Feature Extraction (MFCC):**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0piy2DEG9tU"
      },
      "outputs": [],
      "source": [
        "def mfcc_feature_extraction(X):\n",
        "  mfcc_features = []\n",
        "  hop_length = 512\n",
        "  sampling_rate_per_sec = 22050\n",
        "  num_segments = 10\n",
        "  audio_duration = 30\n",
        "  sampling_rate_for_audio = sampling_rate_per_sec * audio_duration\n",
        "  num_samples_per_segment = sampling_rate_for_audio / num_segments\n",
        "  f_length = math.ceil(num_samples_per_segment / hop_length)\n",
        "\n",
        "  # For each 3 second audio file, getting 20 mfcc features each\n",
        "  for i in range(len(X)):\n",
        "\n",
        "    # Returns a numpy array with the shape as [n_mfcc, number of samples per audio file / hop length]\n",
        "    # (number of samples per audio file / hop length) => gives the number of frames that the FFT is applied to\n",
        "    # The shape of f => (20, 130)\n",
        "    f = librosa.feature.mfcc(y = X[i], sr = sampling_rate_per_sec, n_mfcc = 20, n_fft = 2048, hop_length = 512)\n",
        "    f = f.T\n",
        "\n",
        "    if f.shape[0] == f_length:\n",
        "      mfcc_features.append(f.tolist())\n",
        "  return np.array(mfcc_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqBIBsVxKLiS"
      },
      "outputs": [],
      "source": [
        "# X_train shape => (7128, 130, 20)\n",
        "X_train = mfcc_feature_extraction(trainX)\n",
        "y_train = trainy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUHPDutYIV22"
      },
      "outputs": [],
      "source": [
        "# X_val shape => (1782, 130, 20)\n",
        "X_val = mfcc_feature_extraction(valX)\n",
        "y_val = valy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xoE6ceE8IZcx"
      },
      "outputs": [],
      "source": [
        "# X_val shape => (990, 130, 20)\n",
        "X_test = mfcc_feature_extraction(testX)\n",
        "y_test = testy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfjgIy2VK7dP"
      },
      "source": [
        "# **Network 1:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBpNMNqnLADY"
      },
      "outputs": [],
      "source": [
        "# Defining an LSTM model using keras\n",
        "\n",
        "model1 = Sequential()\n",
        "\n",
        "model1.add(LSTM(128, input_shape=(X_train.shape[1:]), return_sequences=True))\n",
        "\n",
        "model1.add(LSTM(128, activation = \"relu\"))\n",
        "\n",
        "model1.add(Dense(64, activation = \"relu\"))\n",
        "\n",
        "model1.add(Dense(10, activation = \"softmax\"))\n",
        "\n",
        "optimiser = tf.keras.optimizers.Adam(learning_rate = 0.0001)\n",
        "model1.compile(optimizer = optimiser, loss = \"SparseCategoricalCrossentropy\", metrics = [\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uX61CmlmLOx-"
      },
      "outputs": [],
      "source": [
        "history = model1.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 32, epochs = 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUm6kPwYLU3l"
      },
      "source": [
        "**Visualisation of the Loss and Accuracy of the model:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6QLQPekLT18"
      },
      "outputs": [],
      "source": [
        "def plots(history):\n",
        "\n",
        "  # Accuracy\n",
        "  train_accuracy = history.history['accuracy']\n",
        "  validation_accuracy = history.history['val_accuracy']\n",
        "\n",
        "  # Loss\n",
        "  train_loss = history.history['loss']\n",
        "  val_loss = history.history['val_loss']\n",
        "\n",
        "  epochs = []\n",
        "  for i in range(1, len(train_accuracy)+1):\n",
        "    epochs.append(i)\n",
        "\n",
        "  # Plotting the Accuracy\n",
        "  plt.plot(epochs, train_accuracy, label = \"Training Accuracy\")\n",
        "  plt.plot(epochs, validation_accuracy, label = \"Validation Accuracy\")\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.xlabel(\"Accuracy\")\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "  print()\n",
        "  print()\n",
        "\n",
        "  # Plotting the Loss\n",
        "  plt.plot(epochs, train_loss, label = \"Training Loss\")\n",
        "  plt.plot(epochs, val_loss, label = \"Validation Loss\")\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.xlabel(\"Loss\")\n",
        "  plt.legend()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxHCBj-WLuyX"
      },
      "outputs": [],
      "source": [
        "plots(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGIJZ4e9Lv7C"
      },
      "outputs": [],
      "source": [
        "# Testing the performance of the model\n",
        "score1 = model1.evaluate(X_test, y_test)\n",
        "print(\"Test Loss = \", score1[0])\n",
        "print(\"Test Accuracy = \", score1[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ucv4AieMMVR4"
      },
      "source": [
        "# **Network 2:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFoMsWj8MdeK"
      },
      "source": [
        "## **1. Conditional Generative Adversarial Network (CGAN) for augementing audio samples**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjJwmINSMwM3"
      },
      "source": [
        "**1.1. Defining the Discriminator:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blXX8jcPMUdD"
      },
      "outputs": [],
      "source": [
        "# The Discriminator for Conditional GAN's will learn on both input and labels\n",
        "# The output of the Discriminator => (32, 1); labelling whether the input is true or false\n",
        "def discriminator(in_shape = 66150):\n",
        "\n",
        "    embedding_dimension = 16\n",
        "    num_classes = 10\n",
        "\n",
        "    # Defining the input audio and passing it to the model\n",
        "    model = Sequential()\n",
        "    # The input of this layer will be 32 audio files + their labels\n",
        "    # Each of these audio files will have a duration of 3 seconds = 66150 samples for 3 seconds\n",
        "    # The label have a shape => (32, 16)\n",
        "    # Input shape = (32, 66166)\n",
        "    model.add(Dense(20))\n",
        "    model.add(LeakyReLU(alpha = 0.1))\n",
        "    # Output Shape = (32, 20)\n",
        "    # Input shape = (32, 20)\n",
        "    model.add(Dense(20))\n",
        "    model.add(LeakyReLU(alpha = 0.1))\n",
        "    # Output shape = (32, 20)\n",
        "    # Input shape = (32, 20)\n",
        "    model.add(Dense(20))\n",
        "    model.add(LeakyReLU(alpha = 0.1))\n",
        "    # Output shape = (32, 20)\n",
        "    # Input shape = (32, 20)\n",
        "    model.add(Dense(1, activation = \"sigmoid\"))\n",
        "    # Output shape = (32, 1)\n",
        "\n",
        "    # Audio input of shape => (32, 66150)\n",
        "    audio_input = Input(shape = in_shape)\n",
        "\n",
        "    # Label input of shape => (32, 10)\n",
        "    label = Input(shape=(num_classes,), dtype='int32')\n",
        "\n",
        "    # Defining the labels\n",
        "    # Input shape = (32, 10)\n",
        "    label_embedding = Dense(16)(label)\n",
        "    # Output shape = (32, 16)\n",
        "\n",
        "    # The Discriminator will take both the audio and labels as input\n",
        "    # audio_input => (32, 66150)\n",
        "    # label_embedding => (32, 16)\n",
        "    # concat shape => (32, 66166)\n",
        "    concat = Concatenate(axis=-1)([audio_input, label_embedding])\n",
        "\n",
        "    pred = model(concat)\n",
        "\n",
        "    return Model([audio_input, label], pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqoCQmC7NBYj"
      },
      "source": [
        "**1.2. Defining the Generator:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsqLCxapNAbo"
      },
      "outputs": [],
      "source": [
        "# Output of the generator will be (batch_size, number of samples for a 3 second audio file) => (32, 66150)\n",
        "def generator(latent_dimension):\n",
        "\n",
        "    embedding_dimension = 16\n",
        "    num_classes = 10\n",
        "\n",
        "    # Defining the generator\n",
        "    model = Sequential()\n",
        "    # Input will be noise and label\n",
        "    # Noise shape => (32, 64)\n",
        "    # Label shape => (32, 16)\n",
        "    # Input to model => (32, 80)\n",
        "    model.add(Dense(20, input_shape = (latent_dimension + embedding_dimension, )))\n",
        "    model.add(LeakyReLU(alpha = 0.1))\n",
        "    # Output shape => (32, 20)\n",
        "    # Input shape => (32, 20)\n",
        "    model.add(Dense(20))\n",
        "    model.add(LeakyReLU(alpha = 0.1))\n",
        "    # Output shape => (32, 20)\n",
        "    # Input shape => (32, 20)\n",
        "    model.add(Dense(20))\n",
        "    model.add(LeakyReLU(alpha = 0.1))\n",
        "    # Output shape => (32, 20)\n",
        "    # Input shape => (32, 20)\n",
        "    model.add(Dense(66150))\n",
        "    # Output shape => (32, 66150)\n",
        "\n",
        "    # Noise\n",
        "    noise = Input(shape=(latent_dimension, ))\n",
        "\n",
        "    # Label input of shape => (32, 10)\n",
        "    label = Input(shape=(num_classes,), dtype='int32')\n",
        "\n",
        "    # Defining the labels\n",
        "    # Input shape = (32, 10)\n",
        "    label_embedding = Dense(16)(label)\n",
        "    # Output shape = (32, 16)\n",
        "\n",
        "    # noise (32, 64) + label (32, 16) = output (32, 80)\n",
        "    concat = Concatenate()([noise, label_embedding])\n",
        "\n",
        "    pred = model(concat)\n",
        "\n",
        "    # This is given as the input to the model\n",
        "    return Model([noise, label], pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sS1P90K1Ndlg"
      },
      "source": [
        "**1.3. Building the Model:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8dB_HWlNfrY"
      },
      "outputs": [],
      "source": [
        "latent_dimension = 64\n",
        "\n",
        "# Build the discriminator\n",
        "dis_model = discriminator()\n",
        "dis_model.compile(loss = \"binary_crossentropy\", metrics = [\"accuracy\"], optimizer = Adam())\n",
        "\n",
        "# Build the generator\n",
        "gen_model = generator(latent_dimension)\n",
        "\n",
        "# Input to the Generator\n",
        "# 1. Labels\n",
        "gen_label = Input(shape = (10, ))\n",
        "# 2. Noise\n",
        "gen_input = Input(shape = (latent_dimension, ))\n",
        "\n",
        "# Output of the discriminator\n",
        "gen_pred = gen_model([gen_input, gen_label])\n",
        "\n",
        "# The discriminator is not being trained here\n",
        "dis_model.trainable = False\n",
        "\n",
        "# The output of the generator is given as the input to the discriminator\n",
        "dis_model_pred = dis_model([gen_pred, gen_label])\n",
        "\n",
        "# When building a combined GAN model, we want the generator to produce audio to trick the discriminator\n",
        "# So the Discriminator is not trained for the Combined GAN model\n",
        "# And the generator will be trained and the weights will be updated\n",
        "c_gan = Model([gen_input, gen_label], dis_model_pred)\n",
        "c_gan.compile(loss = \"binary_crossentropy\", optimizer = Adam())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGH2JhA_NN7s"
      },
      "source": [
        "**1.4. Training the GAN:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdhTFsxHTVz9"
      },
      "outputs": [],
      "source": [
        "loss = []\n",
        "accuracy = []\n",
        "X = trainX\n",
        "new_y = []\n",
        "for val in trainy:\n",
        "  new_y.append(classes[val])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nm3wz-e1NJzJ"
      },
      "outputs": [],
      "source": [
        "# Using one hot encoding for the labels passed to train the GAN\n",
        "l_encoder = LabelEncoder()\n",
        "int_encoder = l_encoder.fit_transform(new_y)\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "Y = onehot_encoder.fit_transform(int_encoder.reshape(len(int_encoder), 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6Q1DemcNVtm"
      },
      "outputs": [],
      "source": [
        "def training(input_features, batch_size, epochs, out_features, discriminator, generator, latent_dimension):\n",
        "\n",
        "    batches_per_epoch = int(X.shape[0] / batch_size)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for batch_num in range(batches_per_epoch):\n",
        "\n",
        "            # Training the Discriminator\n",
        "            # Trained on both real and fake data\n",
        "            # Real data with shape => (32, 66150), where 32 is the batch size\n",
        "            X_real_train = X[batch_num * batch_size:(batch_num * batch_size) + batch_size, :]\n",
        "            # Shape => (32, 10)\n",
        "            y_real_train_labels = Y[batch_num * batch_size:(batch_num * batch_size) + batch_size, :]\n",
        "\n",
        "            # The fake data is produced by the generator\n",
        "            noise = np.random.normal(loc = 0, scale = 1, size = (batch_size, latent_dimension))\n",
        "            # Shape => (32, 66150) => fake audio\n",
        "            X_fake_train = gen_model.predict([noise, y_real_train_labels])\n",
        "\n",
        "            # Since we are trying to fool the Discriminator, the fake data will also be given a label of 1\n",
        "            real_label = np.ones((X_real_train.shape[0], 1))\n",
        "            fake_label = np.ones((X_real_train.shape[0], 1))\n",
        "\n",
        "            # Training the discriminator\n",
        "            dis_real_loss = dis_model.train_on_batch([X_real_train, y_real_train_labels], np.ones((batch_size, 1)))\n",
        "            dis_fake_loss = dis_model.train_on_batch([X_fake_train, y_real_train_labels], np.zeros((batch_size, 1)))\n",
        "            dis_loss = 0.5 * np.add(dis_real_loss, dis_fake_loss)\n",
        "\n",
        "            # Training the Generator, which means using the combined model, where the Discriminator is not training\n",
        "            noise = np.random.normal(loc = 0, scale = 1, size = (batch_size, latent_dimension))\n",
        "            cgan_loss = c_gan.train_on_batch([noise, y_real_train_labels], real_label)\n",
        "\n",
        "            loss.append((dis_loss[0], cgan_loss))\n",
        "            accuracy.append(dis_loss[1])\n",
        "            print(\"Epoch: \", epoch, \" ; Batch No: \", batch_num, \"  ; d_loss: \", dis_loss[0], \" ; dis_acc: \", 100 * dis_loss[1], \" ; gan_loss: \", cgan_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0SRHuY6NcPE"
      },
      "outputs": [],
      "source": [
        "training(input_features = X.shape[1],\n",
        "         batch_size = 32,\n",
        "         epochs = 50,\n",
        "         out_features = X.shape[1],\n",
        "         discriminator = dis_model,\n",
        "         generator = gen_model,\n",
        "         latent_dimension = latent_dimension\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_Z3zk9CNn94"
      },
      "source": [
        "**1.5. Visualising the Generator and Discriminator Loss:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "555QBHSgNoP2"
      },
      "outputs": [],
      "source": [
        "# length should be 50\n",
        "dis_loss_epoch = []\n",
        "gen_loss_epoch = []\n",
        "for i in range(221, len(loss), 221):\n",
        "  dis_loss_epoch.append(loss[i][0])\n",
        "  gen_loss_epoch.append(loss[i][1])\n",
        "\n",
        "epochs = []\n",
        "for i in range(50):\n",
        "  epochs.append(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crr-1mkiNtJn"
      },
      "outputs": [],
      "source": [
        "plt.plot(epochs, dis_loss_epoch, label = \"Discriminator Loss\")\n",
        "plt.plot(epochs, gen_loss_epoch, label = \"Generator Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Loss per Epoch\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaSKejSyOFT9"
      },
      "source": [
        "**1.6. Saving the audio files to the drive:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfQzHSjbY8GI"
      },
      "outputs": [],
      "source": [
        "# Changing the current working directory to the location on the drive\n",
        "%cd \"/content/drive/MyDrive/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jx2yKyFzX4wK"
      },
      "outputs": [],
      "source": [
        "# Creating the directories\n",
        "!mkdir -p Augmented_Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Bxfha4AZUwk"
      },
      "outputs": [],
      "source": [
        "# Changing the current working directory to the location of the new folder\n",
        "%cd \"/content/drive/MyDrive/Augmented_Data\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIQpWmoPZ05F"
      },
      "outputs": [],
      "source": [
        "# Creating the sub-directories for each class\n",
        "!mkdir -p blues\n",
        "!mkdir -p classical\n",
        "!mkdir -p country\n",
        "!mkdir -p disco\n",
        "!mkdir -p hiphop\n",
        "!mkdir -p jazz\n",
        "!mkdir -p metal\n",
        "!mkdir -p pop\n",
        "!mkdir -p reggae\n",
        "!mkdir -p rock"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCEVX32qOFKI"
      },
      "outputs": [],
      "source": [
        "# Creating fake audio samples (duration = 3 seconds) and putting them in a separate folder\n",
        "label_vectors = np.identity(10)\n",
        "dir_path = \"/content/drive/MyDrive/Augmented_Data\"\n",
        "\n",
        "for i in range(label_vectors.shape[0]):\n",
        "  inverted = l_encoder.inverse_transform([argmax(label_vectors[i])])\n",
        "  label = inverted[0]\n",
        "  dir_name = dir_path + \"/\" + label + \"/\" + label\n",
        "\n",
        "  for j in range(1000):\n",
        "    name = dir_name + \".\" + str(j) + \".wav\"\n",
        "    noise = np.random.normal(loc = 0, scale = 1, size = (1, 64))\n",
        "    audio_time_series = gen_model.predict([noise, (label_vectors[i]).reshape(1, -1)])\n",
        "    sf.write(name, np.ravel(audio_time_series), 22050, format = \"wav\", subtype = \"PCM_16\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruanQ1vobXdy"
      },
      "outputs": [],
      "source": [
        "# Changing the current working directory back to the original\n",
        "!cd \"/content\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVFbwC-tQkU3"
      },
      "source": [
        "## **2. Building Network 2:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73FwpTRCP7lj"
      },
      "outputs": [],
      "source": [
        "# Path to the augemented dataset\n",
        "path1 = \"/content/drive/MyDrive/Augmented_Data\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXIxRNbs6kWi"
      },
      "outputs": [],
      "source": [
        "X_train = list(X_train)\n",
        "y_train = list(y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_D86NuDTA1J"
      },
      "source": [
        "**2.1. Reading the new 3 second audio files generated by the GAN and extracting the features:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-er5G--8htH9"
      },
      "outputs": [],
      "source": [
        "hop_length = 512\n",
        "sampling_rate_per_sec = 22050\n",
        "num_segments = 10\n",
        "audio_duration = 30\n",
        "sampling_rate_for_audio = sampling_rate_per_sec * audio_duration\n",
        "num_samples_per_segment = sampling_rate_for_audio / num_segments\n",
        "f_length = math.ceil(num_samples_per_segment / hop_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdxul3b1RGct"
      },
      "outputs": [],
      "source": [
        "for rootdir, subdir, files in os.walk(path1):\n",
        "  if rootdir != path1:\n",
        "\n",
        "    genre = rootdir.split(\"/\")[-1]\n",
        "    label = classes.index(genre)\n",
        "\n",
        "    for f in files:\n",
        "      filepath = os.path.join(rootdir, f)\n",
        "      waveform, sample_rate = librosa.load(filepath)\n",
        "\n",
        "      # Returns a numpy array with the shape as [n_mfcc, ]\n",
        "      # number of samples per second / hop length => which gives the number of frames that the FFT is applied to\n",
        "      f = librosa.feature.mfcc(y = waveform, sr = sampling_rate_per_sec, n_mfcc = 20, n_fft = 2048, hop_length = hop_length)\n",
        "      f = f.T\n",
        "\n",
        "      if f.shape[0] == f_length:\n",
        "        X_train.append(f.tolist())\n",
        "        y_train.append(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4ZH1RzQiSSb"
      },
      "outputs": [],
      "source": [
        "# X_train shape =>(17128, 130, 20)\n",
        "X_train = np.array(X_train)\n",
        "# y_train shape => (17128,)\n",
        "y_train = np.array(y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61q2YUwzUGmK"
      },
      "source": [
        "**2.2. LSTM with augmented Data:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0WTrsM0T3jy"
      },
      "outputs": [],
      "source": [
        "model2 = Sequential()\n",
        "\n",
        "model2.add(LSTM(128, input_shape=(X_train.shape[1:]), return_sequences=True))\n",
        "\n",
        "model2.add(LSTM(128, activation='relu'))\n",
        "\n",
        "model2.add(Dense(64, activation='relu'))\n",
        "\n",
        "model2.add(Dense(10, activation='softmax'))\n",
        "\n",
        "optimiser = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "model2.compile(optimizer=optimiser, loss='SparseCategoricalCrossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0RUbIOhT-sT"
      },
      "outputs": [],
      "source": [
        "history = model2.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 64, epochs = 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aVEr2MCUKNs"
      },
      "source": [
        "**2.3. Visualisation of the Accuracy and the Loss:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzVS0hYEUQYu"
      },
      "outputs": [],
      "source": [
        "plots(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFWQlyl1U3gR"
      },
      "outputs": [],
      "source": [
        "# Testing the performance of the model\n",
        "score2 = model2.evaluate(X_test, y_test)\n",
        "print(\"Test Loss = \", score2[0])\n",
        "print(\"Test Accuracy = \", score2[1])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}